# Data Engineering Challenge
## Introduction
This repository contains my solution for the technical challenge proposed related to the COVID-19 Kaggle challenge. The proposed solution runs on top of the challenge databricks cluster and it is composed of several python notebooks. Even if all notebooks are stored in my workspace in the challenge cluster, you can also find the here. Next sections will elaborate on requirements, designe and how to run the solution.
## Requirements
In order to run the challenge solution, a databricks 6.6 cluster is needed. On top of that, there is also need to install following libraries:
-  elasticsearch_spark_20_2_11_7_9_3 in order to connect spark to elasticsearch engine.
-  graphframes_0_8_1_spark2_4_s_2_11 in order to create authors graph.
- [langdetect](https://pypi.org/project/langdetect/) in order to identify document language.
## How to run the software
In order to run the solution you just need to copy a batch of documents into dbfs folder /FileStore/tables/cfuentes/input_test. As ChallengeWorkflow notebook has been scheduled to run every 15 minutes, files on that folder should be eventually processed. In any case, you can always launch ChallengeWorkflo manually.
## Solution description
Solution implementation has been designed to work over the databricks challenge cluster and takes advantage of several of it's features such as notebooks, workflows, jobs and spark integration. ChallengeWorkflow notebook contains the implementation of the process flow. This workflow runs every 15 minutes, processing all json files present at a given location. Following diagram describes that processing:

![Workflow diagram](images/Workflow.jpg)

A brief description of the workflow componentes can be found below.

### FilterNonEnglishDocuments
At this first step, the goal is to filter out all non english documents. In order to do that, a python notebook has been develop. This notebook uses pyspark to run a spark process that reads all json in a given location and applys a user defined function over the documents bodies in order to identify their languages. This user defined function uses python library langdetect in order to detect a text language. Once the documents language has been identified, non english documents are moved to error folder and english documents are written to a parquet file in order to continue with its processing.
### ElasticSearchIndexing
At this step of the process, documents written into parquet are indexed into an elasticsearch index. To that end, a python notebook that makes use of elasticsearch_spark library has been developed.  This notebook loads parquet document into spark, applies some spark-ml functions in order to tokenize the abstracts and remove stopwords and, finally, it writes the documents into elasticsearch.
Besides that, a file containing information related to word frequency is also generated. This has been considered a better option that use elasticsearch aggregations (that can be quite expensive in terms of memory and computing time) to get word frequency information. This information will be used during visualization stages.
As a result of this process, a new column is added to parquet file in order to keep record of possible errors ocurred during elastic search indexing. Nowdays, only documents with no abstract are marked as failed. This information will be used in later stages of the processing. 
### GraphAnalysis
At this step of the flow, a graph of author relationships are built based in article co-authorships. In this way, two authors are conected if they have written an article together.
In order to build such a graph, graphframes library has been used. GraphAnalysis python notebook loads english documents parquet file into spark and computes the edges and vertexs of the co-authorship graph. Once the graph is built, we get inDegree information as a meassure of popularity and save into a results file. This file will be used in the visualization module of this solution.
Finally, as a result of this process, a new column is added to parquet to keep record of possible errors ocurred during graph analysis phase. At this time, only documents with no authors information are marked as failed. This information will be used by ProcessCheck notebook to move documents to proper folders after processing ends.
### ProcessCheck
ProcessCheck notebooks uses information generated by GraphAnalysis and ElasticSearchIndexing phases in order to move processed documents to error or processed folders. 
### Visualizations
Finally, based on the outcomes of the implemented workflow, two visualizations have been developed according to challenge requirements. This visualizations are build on top of databricks tables created from process output files. Once tables are created, two queries are done and displayed, one for the ten more influential authors and another for the most frequent terms.
